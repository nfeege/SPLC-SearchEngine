{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling / LDA\n",
    "\n",
    "based on tutorial found at\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "Note: The Oathkeeper data set is >250,000 lines. Reading it takes a couple minutes, so be patient. Processing the whole dataset takes a lot longer, so this example uses only a subset of the datafile to test the analysis flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare cleanup function\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare function to print top words in cleaned data\n",
    "import collections\n",
    "\n",
    "def count_words( row ):\n",
    "    \n",
    "    wlist = row['post_clean']\n",
    "    dcount = collections.Counter(wlist).most_common(ncount)\n",
    "\n",
    "    print(\"Word count in post: \" , row.index.name )\n",
    "    print(dcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data do some clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_pickle(\"nationalOathKeepers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove trailing whitespace in post_forum\n",
    "df['post_forum'] = df['post_forum'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of original dataframe:  (257341, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of original dataframe: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataframe by 'thread_name' to merge discussion in single thread into one 'text sample'\n",
    "df2 = df.groupby('thread_name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of grouped-by dataframe:  (23528, 5)\n"
     ]
    }
   ],
   "source": [
    "# print size of grouped dataframe\n",
    "print(\"Size of grouped-by dataframe: \", df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column counting the number of words in the post_clean column\n",
    "# This is a measure of thread length and allows to exclude too short threads\n",
    "# from topic analysis\n",
    "df2['word_count'] = [ len(x) for x in df2['post_content'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only rows with a certain minimum number of words in content\n",
    "df2sample = df2[ df2['word_count'] > 100000 ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only subset of rows (speeding things up for testing purpose)\n",
    "nrows = 10\n",
    "df2sample = df2sample[:nrows].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "df2sample['post_clean'] = [clean(doc).split() for doc in df2sample['post_content'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up part 2: Remove most frequent words, stop words, etc\n",
    "# @TDODO implement this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency analysis\n",
    "\n",
    "Doing a simple frequency count of words in each thread in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a ranked list of the most frequent words occuring in each thread fomr the cleaned up data subset (each row corresponds to a single thread with all posts from that thread grouped together):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count in post:  None\n",
      "[('gun', 170), ('mental', 151), ('health', 133), ('control', 105), ('article', 97)]\n",
      "Word count in post:  None\n",
      "[('view', 747), ('police', 437), ('post', 434), ('thread', 388), ('forum', 321)]\n",
      "Word count in post:  None\n",
      "[('view', 299), ('post', 158), ('target', 137), ('terrorist', 108), ('please', 107)]\n",
      "Word count in post:  None\n",
      "[('view', 3212), ('oath', 3071), ('keeper', 2850), ('post', 1736), ('coin', 1386)]\n",
      "Word count in post:  None\n",
      "[('bus', 247), ('harrisburg', 246), ('pa', 231), ('reply', 204), ('pm', 197)]\n",
      "Word count in post:  None\n",
      "[('view', 384), ('9mm', 208), ('post', 197), ('found', 138), ('levoy', 129)]\n",
      "Word count in post:  None\n",
      "[('muslim', 227), ('friend', 160), ('battle', 150), ('disengaging', 144), ('reply', 144)]\n",
      "Word count in post:  None\n",
      "[('dc', 334), ('may', 331), ('march', 305), ('reply', 274), ('re', 259)]\n",
      "Word count in post:  None\n",
      "[('benghazi', 10415), ('attack', 7411), ('u', 4951), ('libya', 4540), ('state', 4394)]\n",
      "Word count in post:  None\n",
      "[('people', 123), ('request', 113), ('intended', 113), ('sincere', 110), ('reply', 110)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top words in cleaned data\n",
    "df2sample.apply(count_words, axis=1)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the most frequent words in each thread are characteristic of the topic of a given thread. There are still forum or language specific words in these lists (like 'reply', 'pm', etc) that should be removed in the clean-up step above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LDA Topic Modeling algorithm\n",
    "\n",
    "Using the LDA Topic Modeling library to try Topic Modeling. No tuning of parameters liek number of topics yet. Just running an example as 'black-box'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim as preparatin for Topic Modeling\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(df2sample['post_clean'])\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df2sample['post_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Topic Modeling\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.038*\"view\" + 0.026*\"oath\" + 0.024*\"keeper\" + 0.021*\"post\" + 0.013*\"re\" + 0.011*\"coin\" + 0.011*\"forum\" + 0.010*\"join\" + 0.010*\"silver\" + 0.010*\"date\"'),\n",
       " (1,\n",
       "  '0.009*\"reply\" + 0.008*\"re\" + 0.008*\"may\" + 0.008*\"dc\" + 0.007*\"march\" + 0.007*\"oath\" + 0.006*\"people\" + 0.005*\"keeper\" + 0.005*\"one\" + 0.005*\"it\"'),\n",
       " (2,\n",
       "  '0.009*\"reply\" + 0.008*\"re\" + 0.006*\"state\" + 0.005*\"one\" + 0.005*\"muslim\" + 0.005*\"bus\" + 0.005*\"harrisburg\" + 0.005*\"people\" + 0.005*\"would\" + 0.005*\"pa\"'),\n",
       " (3,\n",
       "  '0.016*\"benghazi\" + 0.012*\"attack\" + 0.008*\"u\" + 0.007*\"libya\" + 0.007*\"state\" + 0.006*\"said\" + 0.005*\"security\" + 0.005*\"one\" + 0.005*\"american\" + 0.004*\"obama\"'),\n",
       " (4,\n",
       "  '0.000*\"benghazi\" + 0.000*\"attack\" + 0.000*\"libya\" + 0.000*\"state\" + 0.000*\"u\" + 0.000*\"said\" + 0.000*\"re\" + 0.000*\"obama\" + 0.000*\"american\" + 0.000*\"reply\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print output\n",
    "ldamodel.print_topics(num_topics=5, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As found in the frequency analysis before, there are still words that should be removed from the text samples because they are language or forum specific ('re', 'reply', ...) but are not related to the actual content or topic.\n",
    "\n",
    "The topics and associated words listed above don't seem like a useful set of topics. How can we better estimate the number of topics to be found in the text samples? Does using more text samples help? Are there any other paramters we can tune for the LDA algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
